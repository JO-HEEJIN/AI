{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOTrSpP2/Lv6jzqRuJG58TX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JO-HEEJIN/AI/blob/master/DeepSeek%2BT5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofsTugU-9BLC",
        "outputId": "d94624c2-419a-447b-bd77-46e5eb4fc16a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers datasets evaluate rouge_score scikit-learn nltk safetensors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install bitsandbytes>=0.39.0\n",
        "# !pip install --upgrade accelerate transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNO6bDMw9vD6",
        "outputId": "8870a2b1-fae8-4873-ea93-648d8ffa99b9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.28.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # GPU 메모리 초기화\n",
        "# cuda.empty_cache()\n",
        "# torch.backends.cuda.matmul.allow_tf32 = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "znmbJ7QKAilf",
        "outputId": "cce1e0fa-437b-4dfb-da93-413a293f5847"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'cuda' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ac9cc92914ef>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# GPU 메모리 초기화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_tf32\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cuda' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/deepseek-ai/DeepSeek-VL.git\n",
        "%cd DeepSeek-VL\n",
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhyUGgA0DwID",
        "outputId": "8fe24837-958b-4845-e242-d34a86695c52"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DeepSeek-VL' already exists and is not an empty directory.\n",
            "/content/DeepSeek-VL\n",
            "Obtaining file:///content/DeepSeek-VL\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from deepseek_vl==1.0.0) (2.5.1+cu124)\n",
            "Requirement already satisfied: transformers>=4.38.2 in /usr/local/lib/python3.11/dist-packages (from deepseek_vl==1.0.0) (4.48.3)\n",
            "Requirement already satisfied: timm>=0.9.16 in /usr/local/lib/python3.11/dist-packages (from deepseek_vl==1.0.0) (1.0.14)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from deepseek_vl==1.0.0) (1.3.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from deepseek_vl==1.0.0) (0.2.0)\n",
            "Requirement already satisfied: attrdict in /usr/local/lib/python3.11/dist-packages (from deepseek_vl==1.0.0) (2.0.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deepseek_vl==1.0.0) (0.8.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm>=0.9.16->deepseek_vl==1.0.0) (0.20.1+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm>=0.9.16->deepseek_vl==1.0.0) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm>=0.9.16->deepseek_vl==1.0.0) (0.28.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm>=0.9.16->deepseek_vl==1.0.0) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.1->deepseek_vl==1.0.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.1->deepseek_vl==1.0.0) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.2->deepseek_vl==1.0.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.2->deepseek_vl==1.0.0) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.2->deepseek_vl==1.0.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.2->deepseek_vl==1.0.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.2->deepseek_vl==1.0.0) (0.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.38.2->deepseek_vl==1.0.0) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->deepseek_vl==1.0.0) (5.9.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from attrdict->deepseek_vl==1.0.0) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.1->deepseek_vl==1.0.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.38.2->deepseek_vl==1.0.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.38.2->deepseek_vl==1.0.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.38.2->deepseek_vl==1.0.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.38.2->deepseek_vl==1.0.0) (2025.1.31)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm>=0.9.16->deepseek_vl==1.0.0) (11.1.0)\n",
            "Building wheels for collected packages: deepseek_vl\n",
            "  Building editable for deepseek_vl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepseek_vl: filename=deepseek_vl-1.0.0-0.editable-py3-none-any.whl size=13124 sha256=3c6c9e00d6a1bd894f9873b974a8419a0f583365d89356576fcedc2bb8c5aa43\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2afooydf/wheels/44/31/43/66fa81af30d05383b2e1099038fdf053952ce9b0eeef29f928\n",
            "Successfully built deepseek_vl\n",
            "Installing collected packages: deepseek_vl\n",
            "  Attempting uninstall: deepseek_vl\n",
            "    Found existing installation: deepseek_vl 1.0.0\n",
            "    Uninstalling deepseek_vl-1.0.0:\n",
            "      Successfully uninstalled deepseek_vl-1.0.0\n",
            "Successfully installed deepseek_vl-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. GPU 설정 및 메모리 최적화\n",
        "import torch\n",
        "from torch import cuda\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5TokenizerFast,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoProcessor,\n",
        "    AutoModelForVision2Seq\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "N0l5IRDZ9UwL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3. T5 모델 초기화 (FP16 if GPU is available)\n",
        "def load_t5_model():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 토크나이저 로드\n",
        "    tokenizer = T5TokenizerFast.from_pretrained(\"google/t5-v1_1-small\")\n",
        "\n",
        "    # 모델 로드 (FP16 지원)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\n",
        "        \"google/t5-v1_1-small\",\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,  # GPU 사용 시 FP16\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    return tokenizer, model\n",
        "\n",
        "# ✅ 테스트 실행\n",
        "tokenizer, model = load_t5_model()\n",
        "print(\"🎯 Model & Tokenizer Loaded Successfully!\")\n",
        "print(f\"Model Device: {model.device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-3bZuGD_3Nr",
        "outputId": "580dfa92-84d9-4e4a-9572-a303efea0505"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 Model & Tokenizer Loaded Successfully!\n",
            "Model Device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, T5ForConditionalGeneration, T5TokenizerFast\n",
        "from deepseek_vl.models import VLChatProcessor, MultiModalityCausalLM\n",
        "from deepseek_vl.utils.io import load_pil_images\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# ✅ 1. DeepSeek-VL 모델 로드 (이미지 → 캡션 생성)\n",
        "def load_deepseek_vl():\n",
        "    model_path = \"deepseek-ai/deepseek-vl-1.3b-chat\"\n",
        "\n",
        "    vl_chat_processor = VLChatProcessor.from_pretrained(model_path)\n",
        "    tokenizer = vl_chat_processor.tokenizer\n",
        "\n",
        "    vl_gpt = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
        "    vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
        "\n",
        "    return vl_chat_processor, vl_gpt, tokenizer\n",
        "\n",
        "# # ✅ 2. T5 모델 로드 (캡션 → 키워드 추출)\n",
        "# def load_t5_model():\n",
        "#     model_name = \"google/t5-v1_1-small\"\n",
        "#     tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
        "#     model = T5ForConditionalGeneration.from_pretrained(model_name).cuda().eval()\n",
        "#     return tokenizer, model\n",
        "\n",
        "# ✅ 3. 이미지에서 캡션 생성\n",
        "def generate_caption(image_path, vl_chat_processor, vl_gpt, tokenizer):\n",
        "    conversation = [\n",
        "        {\"role\": \"User\", \"content\": \"<image_placeholder>Describe this image in detail.\", \"images\": [image_path]},\n",
        "        {\"role\": \"Assistant\", \"content\": \"\"}\n",
        "    ]\n",
        "\n",
        "    pil_images = load_pil_images(conversation)\n",
        "    prepare_inputs = vl_chat_processor(conversations=conversation, images=pil_images, force_batchify=True).to(vl_gpt.device)\n",
        "\n",
        "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
        "    outputs = vl_gpt.language_model.generate(\n",
        "        inputs_embeds=inputs_embeds,\n",
        "        attention_mask=prepare_inputs.attention_mask,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=False,\n",
        "        use_cache=True\n",
        "    )\n",
        "\n",
        "    caption = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "# ✅ 4. 캡션에서 키워드 추출\n",
        "def extract_keywords(caption, t5_tokenizer, t5_model):\n",
        "    input_text = f\"extract keywords: {caption}\"\n",
        "    inputs = t5_tokenizer(input_text, return_tensors=\"pt\").to(t5_model.device)\n",
        "\n",
        "    outputs = t5_model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n",
        "    keywords = t5_tokenizer.decode(outputs[0], skip_special_tokens=True).replace(\"keywords:\", \"\").strip()\n",
        "\n",
        "    return keywords\n",
        "\n",
        "# ✅ 5. 전체 워크플로우 실행\n",
        "def process_image(image_path):\n",
        "    print(f\"📌 Processing Image: {image_path}\")\n",
        "\n",
        "    # 모델 로드\n",
        "    vl_chat_processor, vl_gpt, vl_tokenizer = load_deepseek_vl()\n",
        "    t5_tokenizer, t5_model = load_t5_model()\n",
        "\n",
        "    # 이미지 캡션 생성\n",
        "    caption = generate_caption(image_path, vl_chat_processor, vl_gpt, vl_tokenizer)\n",
        "    print(f\"📝 Generated Caption: {caption}\")\n",
        "\n",
        "    # 키워드 추출\n",
        "    keywords = extract_keywords(caption, t5_tokenizer, t5_model)\n",
        "    print(f\"🔑 Extracted Keywords: {keywords}\")\n",
        "\n",
        "    return caption, keywords\n",
        "\n",
        "# # ✅ 6. Colab 파일 업로드 기능 (테스트용)\n",
        "# from google.colab import files\n",
        "\n",
        "# uploaded = files.upload()\n",
        "# for filename in uploaded.keys():\n",
        "#     caption, keywords = process_image(filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "hapBLuuuAs-G",
        "outputId": "7b68aff2-2325-4ed5-c15f-83cdac1f2e88"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-aad1688b-32dd-4741-b276-7f6582688ebf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-aad1688b-32dd-4741-b276-7f6582688ebf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Screenshot 2025-02-13 at 8.30.47 AM.png to Screenshot 2025-02-13 at 8.30.47 AM.png\n",
            "📌 Processing Image: Screenshot 2025-02-13 at 8.30.47 AM.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some kwargs in processor config are unused and will not have any effect: mask_prompt, sft_format, image_tag, ignore_id, add_special_token, num_image_tokens. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📝 Generated Caption: The image captures a close-up of a tiger's face, set against a blurred background that suggests a natural, outdoor setting. The tiger's eyes, a striking shade of green, are wide open, gazing directly into the camera, creating a sense of connection between the viewer and the subject. The tiger's fur is a mix of brown and black stripes, a characteristic feature of this majestic creature. The whiskers, a defining feature of the tiger, are clearly visible, adding to the overall detail of the image. The image does not contain any text or other discernible objects. The tiger's position in the frame and the direct gaze into the camera give the image a dynamic and engaging quality.\n",
            "🔑 Extracted Keywords: .com:.com extract .com extract .com extract .com extract .com extract .com.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5TokenizerFast,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from deepseek_vl.models import VLChatProcessor, MultiModalityCausalLM\n",
        "from deepseek_vl.utils.io import load_pil_images\n",
        "from PIL import Image\n",
        "import os\n",
        "import ast # 안전한 문자열 → 리스트 변환\n",
        "\n",
        "# ✅ 1. DeepSeek-VL 모델 로드 (이미지 → 캡션 생성)\n",
        "def load_deepseek_vl():\n",
        "    model_path = \"deepseek-ai/deepseek-vl-1.3b-chat\"\n",
        "\n",
        "    vl_chat_processor = VLChatProcessor.from_pretrained(model_path)\n",
        "    tokenizer = vl_chat_processor.tokenizer\n",
        "\n",
        "    vl_gpt = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
        "    vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()\n",
        "\n",
        "    return vl_chat_processor, vl_gpt, tokenizer"
      ],
      "metadata": {
        "id": "FFozm2bzUV_p"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5TokenizerFast,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import os\n",
        "import ast\n",
        "\n",
        "# ✅ 1. 안전한 디코딩 함수\n",
        "def safe_decode(tokenizer, token_ids):\n",
        "    valid_token_ids = [t for t in token_ids if 0 <= t < tokenizer.vocab_size]\n",
        "    if not valid_token_ids:\n",
        "        return \"\"\n",
        "    return tokenizer.decode(valid_token_ids, skip_special_tokens=True)\n",
        "\n",
        "# ✅ 2. T5 모델 로드\n",
        "def load_t5_model(fine_tuned_path=\"./fine_tuned_t5\"):\n",
        "    model_name = \"google/t5-v1_1-small\"\n",
        "    tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "    if os.path.exists(fine_tuned_path):\n",
        "        print(\"✅ Loading Fine-Tuned T5 Model\")\n",
        "        model = T5ForConditionalGeneration.from_pretrained(fine_tuned_path).cuda().eval()\n",
        "    else:\n",
        "        print(\"⚠️ Fine-Tuned Model Not Found! Using Pretrained Model Instead.\")\n",
        "        model = T5ForConditionalGeneration.from_pretrained(model_name).cuda().eval()\n",
        "\n",
        "    return tokenizer, model\n",
        "\n",
        "# ✅ 3. 데이터셋 로드 및 전처리\n",
        "def preprocess_function(examples, tokenizer):\n",
        "    inputs = [\"extract keywords: \" + caption for caption in examples[\"caption\"]]\n",
        "    targets = [\"keywords: \" + \", \".join(ast.literal_eval(kws)) for kws in examples[\"keywords\"]]\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    labels = tokenizer(\n",
        "        targets,\n",
        "        max_length=128,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# ✅ 4. ROUGE 평가 함수\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = [safe_decode(tokenizer, pred) for pred in predictions]\n",
        "    decoded_labels = [safe_decode(tokenizer, label) for label in labels]\n",
        "\n",
        "    result = rouge.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=decoded_labels,\n",
        "        use_stemmer=True\n",
        "    )\n",
        "    return {k: round(v, 4) for k, v in result.items()}\n",
        "\n",
        "# ✅ 5. T5 모델 Fine-Tuning\n",
        "def fine_tune_t5():\n",
        "    tokenizer = T5TokenizerFast.from_pretrained(\"google/t5-v1_1-small\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"google/t5-v1_1-small\")\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "    model.config.use_cache = False  # 🔹 FP16 문제 해결\n",
        "\n",
        "    dataset = load_dataset(\"sachithra913/keywordExtraction\")\n",
        "    tokenized_dataset = dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True, remove_columns=[\"caption\", \"keywords\"])\n",
        "\n",
        "    train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(7000))\n",
        "    eval_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(7000, 9000))\n",
        "    test_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(9000, 10000))\n",
        "\n",
        "    # ✅ 학습 설정 (max_new_tokens 삭제)\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=5e-5,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=32,\n",
        "        weight_decay=0.01,\n",
        "        save_total_limit=3,\n",
        "        num_train_epochs=5,\n",
        "        predict_with_generate=True,\n",
        "        generation_max_length=128,  # ✅ max_new_tokens 대신 사용\n",
        "        bf16=True,  # ✅ FP16 문제 해결: FP16 대신 BF16 사용\n",
        "        gradient_checkpointing=True,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        report_to=\"none\",\n",
        "        logging_steps=50,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"rougeL\"\n",
        "    )\n",
        "\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model(\"./fine_tuned_t5\")\n",
        "    print(\"✅ Fine-Tuning 완료! 🎉\")\n",
        "\n",
        "    # ✅ 평가 수행 (여기서 max_new_tokens 적용)\n",
        "    predictions = trainer.predict(eval_dataset, max_new_tokens=128)\n",
        "    print(\"🔎 Prediction Result:\", predictions)\n",
        "\n",
        "# 실행\n",
        "fine_tune_t5()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "ndIlvo7oUgxz",
        "outputId": "84c9a1b3-15e5-4730-9641-bf4c3d270a97"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-40-363d3a9fb2b6>:119: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2190' max='2190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2190/2190 13:26, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.872400</td>\n",
              "      <td>0.332704</td>\n",
              "      <td>0.141500</td>\n",
              "      <td>0.017800</td>\n",
              "      <td>0.141400</td>\n",
              "      <td>0.141700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.217600</td>\n",
              "      <td>0.073464</td>\n",
              "      <td>0.817600</td>\n",
              "      <td>0.651500</td>\n",
              "      <td>0.809400</td>\n",
              "      <td>0.809300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.118300</td>\n",
              "      <td>0.044563</td>\n",
              "      <td>0.919200</td>\n",
              "      <td>0.835000</td>\n",
              "      <td>0.912000</td>\n",
              "      <td>0.912000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.099600</td>\n",
              "      <td>0.036527</td>\n",
              "      <td>0.945500</td>\n",
              "      <td>0.874600</td>\n",
              "      <td>0.938100</td>\n",
              "      <td>0.938200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.092200</td>\n",
              "      <td>0.035292</td>\n",
              "      <td>0.951200</td>\n",
              "      <td>0.884600</td>\n",
              "      <td>0.943900</td>\n",
              "      <td>0.943900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fine-Tuning 완료! 🎉\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Prediction Result: PredictionOutput(predictions=array([[    0, 12545,    10, ...,  -100,  -100,  -100],\n",
            "       [    0, 12545,    10, ...,  -100,  -100,  -100],\n",
            "       [    0, 12545,    10, ...,  -100,  -100,  -100],\n",
            "       ...,\n",
            "       [    0, 12545,    10, ...,  -100,  -100,  -100],\n",
            "       [    0, 12545,    10, ...,  -100,  -100,  -100],\n",
            "       [    0, 12545,    10, ...,  -100,  -100,  -100]]), label_ids=array([[12545,    10,  7178, ...,     0,     0,     0],\n",
            "       [12545,    10,   385, ...,     0,     0,     0],\n",
            "       [12545,    10,   388, ...,     0,     0,     0],\n",
            "       ...,\n",
            "       [12545,    10,  3202, ...,     0,     0,     0],\n",
            "       [12545,    10,  2182, ...,     0,     0,     0],\n",
            "       [12545,    10,   388, ...,     0,     0,     0]]), metrics={'test_loss': 0.03529224172234535, 'test_rouge1': 0.9512, 'test_rouge2': 0.8846, 'test_rougeL': 0.9439, 'test_rougeLsum': 0.9439, 'test_runtime': 43.0226, 'test_samples_per_second': 46.487, 'test_steps_per_second': 1.464})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_caption(image_path, vl_chat_processor, vl_gpt, tokenizer):\n",
        "    conversation = [\n",
        "        {\"role\": \"User\", \"content\": \"<image_placeholder>Describe this image in detail.\", \"images\": [image_path]},\n",
        "        {\"role\": \"Assistant\", \"content\": \"\"}\n",
        "    ]\n",
        "\n",
        "    pil_images = load_pil_images(conversation)\n",
        "    prepare_inputs = vl_chat_processor(conversations=conversation, images=pil_images, force_batchify=True).to(vl_gpt.device)\n",
        "\n",
        "    inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)\n",
        "    outputs = vl_gpt.language_model.generate(\n",
        "        inputs_embeds=inputs_embeds,\n",
        "        attention_mask=prepare_inputs.attention_mask,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=False,\n",
        "        use_cache=True\n",
        "    )\n",
        "\n",
        "    caption = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "# ✅ 4. 캡션에서 키워드 추출\n",
        "def extract_keywords(caption, t5_tokenizer, t5_model):\n",
        "    input_text = f\"extract keywords: {caption}\"\n",
        "    inputs = t5_tokenizer(input_text, return_tensors=\"pt\").to(t5_model.device)\n",
        "\n",
        "    outputs = t5_model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n",
        "    keywords = t5_tokenizer.decode(outputs[0], skip_special_tokens=True).replace(\"keywords:\", \"\").strip()\n",
        "\n",
        "    return keywords\n",
        "\n",
        "# ✅ 5. 전체 워크플로우 실행\n",
        "def process_image(image_path):\n",
        "    print(f\"📌 Processing Image: {image_path}\")\n",
        "\n",
        "    # 모델 로드\n",
        "    vl_chat_processor, vl_gpt, vl_tokenizer = load_deepseek_vl()\n",
        "    t5_tokenizer, t5_model = load_t5_model()\n",
        "\n",
        "    # 이미지 캡션 생성\n",
        "    caption = generate_caption(image_path, vl_chat_processor, vl_gpt, vl_tokenizer)\n",
        "    print(f\"📝 Generated Caption: {caption}\")\n",
        "\n",
        "    # 키워드 추출\n",
        "    keywords = extract_keywords(caption, t5_tokenizer, t5_model)\n",
        "    print(f\"🔑 Extracted Keywords: {keywords}\")\n",
        "\n",
        "    return caption, keywords\n",
        "\n",
        "# ✅ 6. Colab 파일 업로드 기능 (테스트용)\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "for filename in uploaded.keys():\n",
        "    caption, keywords = process_image(filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "lVvm8qVCMNp8",
        "outputId": "9af84336-5d3b-436f-ac69-2459bf632a8b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3b9fc373-4373-43f9-bcde-5447b5fc23ce\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3b9fc373-4373-43f9-bcde-5447b5fc23ce\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Screenshot 2025-02-13 at 8.30.47 AM.png to Screenshot 2025-02-13 at 8.30.47 AM (1).png\n",
            "📌 Processing Image: Screenshot 2025-02-13 at 8.30.47 AM (1).png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some kwargs in processor config are unused and will not have any effect: mask_prompt, sft_format, image_tag, ignore_id, add_special_token, num_image_tokens. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loading Fine-Tuned T5 Model\n",
            "📝 Generated Caption: The image captures a close-up of a tiger's face, set against a blurred background that suggests a natural, outdoor setting. The tiger's eyes, a striking shade of green, are wide open, gazing directly into the camera, creating a sense of connection between the viewer and the subject. The tiger's fur is a mix of brown and black stripes, a characteristic feature of this majestic creature. The whiskers, a defining feature of the tiger, are clearly visible, adding to the overall detail of the image. The image does not contain any text or other discernible objects. The tiger's position in the frame and the direct gaze into the camera give the image a dynamic and engaging quality.\n",
            "🔑 Extracted Keywords: tiger, eyes, wide, open\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "318Y1UCFMb-8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}